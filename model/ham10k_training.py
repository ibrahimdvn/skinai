#!/usr/bin/env python3
"""
ğŸ¯ SkinAI - HAM10000 GeliÅŸmiÅŸ Model EÄŸitimi
================================================
HAM10000 dataset'i kullanarak gÃ¼Ã§lÃ¼ bir cilt kanseri tespit modeli eÄŸitir.

Dataset Mapping:
- mel (melanoma) â†’ melanoma
- nv (nevus) â†’ nevus  
- bkl, bcc, akiec, df â†’ benign

Author: SkinAI Team
Date: 2025-06-28
"""

import os
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
import matplotlib.pyplot as plt
import shutil
from PIL import Image
import json

# KonfigÃ¼rasyon
CONFIG = {
    'IMG_SIZE': 224,
    'BATCH_SIZE': 32,
    'EPOCHS': 40,
    'LEARNING_RATE': 0.001,
    'VALIDATION_SPLIT': 0.2,
    'TEST_SPLIT': 0.1,
    'NUM_CLASSES': 3,
    'MODEL_NAME': 'ham10k_model.h5'
}

class HAM10kTrainer:
    def __init__(self):
        self.metadata_path = 'model/archive/HAM10000_metadata.csv'
        self.images_part1 = 'model/archive/HAM10000_images_part_1'
        self.images_part2 = 'model/archive/HAM10000_images_part_2'
        self.output_dir = 'data/ham10k_processed'
        
        # SÄ±nÄ±f mapping: HAM10000 -> Bizim sistem
        self.class_mapping = {
            'mel': 'melanoma',      # Melanoma
            'nv': 'nevus',          # Nevus (mole)
            'bkl': 'benign',        # Benign keratosis
            'bcc': 'benign',        # Basal cell carcinoma  
            'akiec': 'benign',      # Actinic keratoses
            'df': 'benign',         # Dermatofibroma
            'vasc': 'benign'        # Vascular lesion
        }
        
        self.class_names = ['benign', 'melanoma', 'nevus']
        
    def load_and_prepare_data(self):
        """HAM10000 metadata'sÄ±nÄ± yÃ¼kler ve veriyi hazÄ±rlar"""
        print("ğŸ“Š HAM10000 metadata yÃ¼kleniyor...")
        
        df = pd.read_csv(self.metadata_path)
        print(f"âœ… {len(df)} gÃ¶rÃ¼ntÃ¼ metadata'sÄ± yÃ¼klendi")
        
        print("\nğŸ” Orijinal sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:")
        print(df['dx'].value_counts())
        
        # SÄ±nÄ±flarÄ± mapping yap
        df['mapped_class'] = df['dx'].map(self.class_mapping)
        
        print("\nğŸ¯ Yeni sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:")
        print(df['mapped_class'].value_counts())
        
        # GÃ¶rÃ¼ntÃ¼ dosyalarÄ±nÄ±n varlÄ±ÄŸÄ±nÄ± kontrol et
        valid_images = []
        for idx, row in df.iterrows():
            image_id = row['image_id']
            
            img_path1 = os.path.join(self.images_part1, f"{image_id}.jpg")
            img_path2 = os.path.join(self.images_part2, f"{image_id}.jpg")
            
            if os.path.exists(img_path1):
                valid_images.append({
                    'image_id': image_id,
                    'image_path': img_path1,
                    'class': row['mapped_class']
                })
            elif os.path.exists(img_path2):
                valid_images.append({
                    'image_id': image_id,
                    'image_path': img_path2,
                    'class': row['mapped_class']
                })
        
        print(f"âœ… {len(valid_images)} geÃ§erli gÃ¶rÃ¼ntÃ¼ bulundu")
        self.df = pd.DataFrame(valid_images)
        return self.df
    
    def organize_dataset(self):
        """Dataset'i train/val/test olarak organize eder"""
        print("\nğŸ“ Dataset organize ediliyor...")
        
        # Output dizinlerini oluÅŸtur
        for split in ['train', 'validation', 'test']:
            for class_name in self.class_names:
                os.makedirs(os.path.join(self.output_dir, split, class_name), exist_ok=True)
        
        # Stratified split
        train_data, temp_data = train_test_split(
            self.df, 
            test_size=CONFIG['VALIDATION_SPLIT'] + CONFIG['TEST_SPLIT'],
            stratify=self.df['class'],
            random_state=42
        )
        
        val_data, test_data = train_test_split(
            temp_data,
            test_size=CONFIG['TEST_SPLIT'] / (CONFIG['VALIDATION_SPLIT'] + CONFIG['TEST_SPLIT']),
            stratify=temp_data['class'],
            random_state=42
        )
        
        # GÃ¶rÃ¼ntÃ¼leri kopyala ve resize et
        splits = {
            'train': train_data,
            'validation': val_data,
            'test': test_data
        }
        
        for split_name, split_df in splits.items():
            print(f"\n{split_name.upper()} seti organize ediliyor...")
            
            for idx, row in split_df.iterrows():
                src_path = row['image_path']
                dst_path = os.path.join(
                    self.output_dir, 
                    split_name, 
                    row['class'], 
                    f"{row['image_id']}.jpg"
                )
                
                try:
                    img = Image.open(src_path)
                    img = img.convert('RGB')
                    img = img.resize((CONFIG['IMG_SIZE'], CONFIG['IMG_SIZE']))
                    img.save(dst_path, 'JPEG', quality=95)
                except Exception as e:
                    print(f"âš ï¸  Hata: {src_path} - {e}")
            
            class_counts = split_df['class'].value_counts()
            print(f"ğŸ“Š {split_name} - Toplam: {len(split_df)}")
            for class_name, count in class_counts.items():
                print(f"   {class_name}: {count}")
        
        print("âœ… Dataset organizasyonu tamamlandÄ±!")
        return train_data, val_data, test_data
    
    def create_data_generators(self):
        """GeliÅŸmiÅŸ data augmentation ile veri generatÃ¶rlerini oluÅŸturur"""
        print("\nğŸ”„ Data generators oluÅŸturuluyor...")
        
        # Training augmentation
        train_datagen = ImageDataGenerator(
            rescale=1./255,
            rotation_range=30,
            width_shift_range=0.2,
            height_shift_range=0.2,
            shear_range=0.2,
            zoom_range=0.2,
            horizontal_flip=True,
            brightness_range=[0.9, 1.1],
            fill_mode='nearest'
        )
        
        # Validation/Test
        val_datagen = ImageDataGenerator(rescale=1./255)
        
        train_generator = train_datagen.flow_from_directory(
            os.path.join(self.output_dir, 'train'),
            target_size=(CONFIG['IMG_SIZE'], CONFIG['IMG_SIZE']),
            batch_size=CONFIG['BATCH_SIZE'],
            class_mode='categorical',
            shuffle=True
        )
        
        validation_generator = val_datagen.flow_from_directory(
            os.path.join(self.output_dir, 'validation'),
            target_size=(CONFIG['IMG_SIZE'], CONFIG['IMG_SIZE']),
            batch_size=CONFIG['BATCH_SIZE'],
            class_mode='categorical',
            shuffle=False
        )
        
        test_generator = val_datagen.flow_from_directory(
            os.path.join(self.output_dir, 'test'),
            target_size=(CONFIG['IMG_SIZE'], CONFIG['IMG_SIZE']),
            batch_size=CONFIG['BATCH_SIZE'],
            class_mode='categorical',
            shuffle=False
        )
        
        # Class weights hesapla
        class_counts = self.df['class'].value_counts()
        total_samples = len(self.df)
        
        class_weights = {}
        for i, class_name in enumerate(self.class_names):
            class_weights[i] = total_samples / (len(self.class_names) * class_counts[class_name])
        
        print(f"ğŸ¯ Class weights: {class_weights}")
        
        return train_generator, validation_generator, test_generator, class_weights
    
    def create_model(self):
        """EfficientNetB0 tabanlÄ± model oluÅŸturur"""
        print("\nğŸ§  Model oluÅŸturuluyor...")
        
        base_model = EfficientNetB0(
            weights='imagenet',
            include_top=False,
            input_shape=(CONFIG['IMG_SIZE'], CONFIG['IMG_SIZE'], 3)
        )
        
        base_model.trainable = False
        
        x = base_model.output
        x = GlobalAveragePooling2D()(x)
        x = BatchNormalization()(x)
        x = Dense(512, activation='relu')(x)
        x = Dropout(0.5)(x)
        x = Dense(256, activation='relu')(x)
        x = Dropout(0.3)(x)
        x = Dense(CONFIG['NUM_CLASSES'], activation='softmax')(x)
        
        model = Model(inputs=base_model.input, outputs=x)
        
        model.compile(
            optimizer=Adam(learning_rate=CONFIG['LEARNING_RATE']),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        print(f"âœ… Model oluÅŸturuldu: {model.count_params():,} parametre")
        return model
    
    def train_model(self, model, train_gen, val_gen, class_weights):
        """Modeli eÄŸitir"""
        print("\nğŸš€ Model eÄŸitimi baÅŸlÄ±yor...")
        
        callbacks = [
            ModelCheckpoint(
                f'model/{CONFIG["MODEL_NAME"]}',
                monitor='val_accuracy',
                save_best_only=True,
                verbose=1
            ),
            EarlyStopping(
                monitor='val_accuracy',
                patience=8,
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=4,
                min_lr=1e-7,
                verbose=1
            )
        ]
        
        # Phase 1: Transfer Learning
        print("ğŸ“š Phase 1: Transfer Learning")
        history1 = model.fit(
            train_gen,
            epochs=15,
            validation_data=val_gen,
            class_weight=class_weights,
            callbacks=callbacks,
            verbose=1
        )
        
        # Phase 2: Fine-tuning
        print("\nğŸ”§ Phase 2: Fine-tuning")
        model.layers[0].trainable = True
        
        model.compile(
            optimizer=Adam(learning_rate=CONFIG['LEARNING_RATE']/10),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        history2 = model.fit(
            train_gen,
            epochs=CONFIG['EPOCHS'] - 15,
            validation_data=val_gen,
            class_weight=class_weights,
            callbacks=callbacks,
            verbose=1
        )
        
        # History birleÅŸtir
        history = {}
        for key in history1.history.keys():
            history[key] = history1.history[key] + history2.history[key]
        
        return model, history
    
    def evaluate_model(self, model, test_gen):
        """Modeli test eder"""
        print("\nğŸ“Š Model deÄŸerlendiriliyor...")
        
        test_loss, test_acc = model.evaluate(test_gen, verbose=1)
        
        print(f"\nğŸ¯ Test SonuÃ§larÄ±:")
        print(f"   Test Loss: {test_loss:.4f}")
        print(f"   Test Accuracy: {test_acc:.4f}")
        
        return test_loss, test_acc
    
    def save_results(self, history, test_results):
        """SonuÃ§larÄ± kaydeder"""
        print("\nğŸ’¾ SonuÃ§lar kaydediliyor...")
        
        # Model bilgileri
        model_info = {
            'model_name': CONFIG['MODEL_NAME'],
            'dataset': 'HAM10000',
            'classes': self.class_names,
            'total_samples': len(self.df),
            'class_distribution': self.df['class'].value_counts().to_dict(),
            'test_accuracy': float(test_results[1]),
            'test_loss': float(test_results[0])
        }
        
        with open('model/ham10k_model_info.json', 'w') as f:
            json.dump(model_info, f, indent=2)
        
        # Training plots
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 2, 1)
        plt.plot(history['accuracy'], label='Train Accuracy')
        plt.plot(history['val_accuracy'], label='Val Accuracy')
        plt.title('Model Accuracy')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend()
        plt.grid(True)
        
        plt.subplot(1, 2, 2)
        plt.plot(history['loss'], label='Train Loss')
        plt.plot(history['val_loss'], label='Val Loss')
        plt.title('Model Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        plt.tight_layout()
        plt.savefig('model/ham10k_training_history.png', dpi=300)
        plt.close()
        
        print("âœ… SonuÃ§lar kaydedildi!")
    
    def run_training(self):
        """Tam eÄŸitim pipeline'Ä±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±r"""
        print("ğŸ¯ HAM10000 GeliÅŸmiÅŸ Model EÄŸitimi")
        print("=" * 50)
        
        try:
            # Pipeline adÄ±mlarÄ±
            self.load_and_prepare_data()
            train_df, val_df, test_df = self.organize_dataset()
            train_gen, val_gen, test_gen, class_weights = self.create_data_generators()
            model = self.create_model()
            trained_model, history = self.train_model(model, train_gen, val_gen, class_weights)
            test_results = self.evaluate_model(trained_model, test_gen)
            self.save_results(history, test_results)
            
            print("\nğŸ‰ HAM10000 model eÄŸitimi tamamlandÄ±!")
            print(f"ğŸ’¾ Model kaydedildi: model/{CONFIG['MODEL_NAME']}")
            print(f"ğŸ“Š Test Accuracy: {test_results[1]:.4f}")
            
            return trained_model, history, test_results
            
        except Exception as e:
            print(f"âŒ EÄŸitim hatasÄ±: {e}")
            raise e

if __name__ == "__main__":
    # GPU ayarlarÄ±
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"ğŸš€ GPU kullanÄ±lÄ±yor: {len(gpus)} GPU")
        except RuntimeError as e:
            print(f"âš ï¸  GPU ayarÄ± hatasÄ±: {e}")
    else:
        print("ğŸ’» CPU kullanÄ±lÄ±yor")
    
    # Training baÅŸlat
    trainer = HAM10kTrainer()
    model, history, results = trainer.run_training() 